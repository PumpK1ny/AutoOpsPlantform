"""QQæœºå™¨äººAIå¯¹è¯æ¨¡å— - æ”¯æŒGLM-4.6Vå¤šæ¨¡æ€ã€å¤šAPIå¯†é’¥ç®¡ç†ã€å·¥å…·è°ƒç”¨"""

import os
import json
import base64
import re
import asyncio
import time
import inspect
from dotenv import load_dotenv
from zhipuai import ZhipuAI
import aiohttp
import message_push.QQ.bot_tool as bot_tool
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥APIå¯†é’¥ç®¡ç†å™¨
from message_push.QQ.api_key_manager import (
    api_key_manager, 
    get_wait_time_estimate,
    create_zhipu_client_with_rotation,
    get_api_key_simple
)

def import_global_functions(module):
    for name, obj in inspect.getmembers(module, inspect.isfunction):
        globals()[name] = obj

import_global_functions(bot_tool)
# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HISTORY_DIR = os.path.join(BASE_DIR, "chat_history")
SYSTEM_PROMPT_FILE = os.path.join(BASE_DIR, "system_prompt.txt")
# å‹ç¼©é˜ˆå€¼ï¼ˆtokenæ•°ï¼Œå¤§çº¦ä¼°ç®—ï¼‰
COMPRESS_THRESHOLD = 10000

# HTTP API é…ç½®
HTTP_API_BASE_URL = os.getenv("QQ_BOT_HTTP_API_URL", "http://localhost:8080")

# ç¡®ä¿å†å²è®°å½•ç›®å½•å­˜åœ¨
os.makedirs(HISTORY_DIR, exist_ok=True)


def load_info(user_openid: str) -> str:
    """ä»æ–‡ä»¶åŠ è½½system prompt"""
    if os.path.exists(SYSTEM_PROMPT_FILE):
        with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
            system_prompt = f.read().strip()
    else:
        system_prompt = "ä½ æ˜¯QQç¾¤é‡Œçš„èŠå¤©æœºå™¨äººï¼Œæ€§æ ¼æ´»æ³¼å¯çˆ±ï¼Œå–œæ¬¢ç”¨è¡¨æƒ…åŒ…ã€‚è¯·ç”¨ç®€æ´å‹å¥½çš„ä¸­æ–‡å›ç­”ã€‚"

    if os.path.exists(os.path.join(BASE_DIR, "bio", f"{user_openid}.json")):
        with open(os.path.join(BASE_DIR, "bio", f"{user_openid}.json"), "r", encoding="utf-8") as f:
            data = json.load(f)
            bio_content = data
    else:
        bio_content = {}
    return system_prompt, bio_content


def load_history(user_openid: str) -> tuple[list, str]:
    """
    åŠ è½½ç”¨æˆ·å†å²å¯¹è¯
    
    Returns:
        (å¯¹è¯è®°å½•åˆ—è¡¨, å‹ç¼©æ‘˜è¦)
    """
    history_file = os.path.join(HISTORY_DIR, f"{user_openid}.json")
    if os.path.exists(history_file):
        with open(history_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data.get("history", []), data.get("summary", "")
            return data, ""
    return [], ""


def save_history(user_openid: str, history: list, summary: str = None):
    """
    ä¿å­˜ç”¨æˆ·å†å²å¯¹è¯
    
    Args:
        history: å¯¹è¯è®°å½•åˆ—è¡¨ï¼ˆä¸åŒ…å«systemï¼‰
        summary: å‹ç¼©æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    """
    history_file = os.path.join(HISTORY_DIR, f"{user_openid}.json")
    
    if summary:
        data = {"history": history, "summary": summary}
    else:
        data = history
    
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def estimate_tokens(text: str) -> int:
    """ä¼°ç®—tokenæ•°"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_words = len(re.findall(r'[a-zA-Z]+', text))
    others = len(text) - chinese_chars - sum(len(w) for w in re.findall(r'[a-zA-Z]+', text))
    return int(chinese_chars * 1.5 + english_words * 1.2 + others * 0.5)


def calculate_context_tokens(context: list) -> int:
    """è®¡ç®—ä¸Šä¸‹æ–‡çš„tokenæ•°"""
    total = 0
    for msg in context:
        content = msg.get("content", "")
        if isinstance(content, str):
            total += estimate_tokens(content)
        elif isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total += estimate_tokens(item.get("text", ""))
                else:
                    total += 1000
    return total


async def _call_zhipu_api_with_rotation(model: str, messages: list, api_params: dict = None) -> any:
    """
    è°ƒç”¨æ™ºè°±APIï¼Œæ”¯æŒå¯†é’¥è½®æ¢
    """
    loop = asyncio.get_event_loop()
    
    if api_params is None:
        api_params = {
            "model": model,
            "messages": messages,
            "max_tokens": int(os.getenv("QQ_BOT_MAX_TOKENS", "1024")),
            "temperature": 0.7,
            "top_p": 0.7,
            "stream": False,
            "thinking": {"type": "disabled"}
        }
    
    # è·å–è½®æ¢å®¢æˆ·ç«¯
    rotating_client = create_zhipu_client_with_rotation()
    
    if rotating_client:
        # ä½¿ç”¨æ”¯æŒè½®æ¢çš„å®¢æˆ·ç«¯
        return await loop.run_in_executor(
            None,
            lambda: rotating_client.chat_completions_create(**api_params)
        )
    else:
        # å›é€€åˆ°æ™®é€šå®¢æˆ·ç«¯
        api_key = get_api_key_simple()
        client = ZhipuAI(api_key=api_key)
        return await loop.run_in_executor(
            None,
            lambda: client.chat.completions.create(**api_params)
        )


class ChatAI:
    """å¤šæ¨¡æ€AIå¯¹è¯ç±» - æ”¯æŒGLM-4.6Vã€å¤šAPIå¯†é’¥ç®¡ç†ã€å·¥å…·è°ƒç”¨"""

    def __init__(self, user_openid: str, msg_api=None):
        self.user_openid = user_openid
        self.msg_api = msg_api
        self.text_model = os.getenv("QQCHAT_TEXT_MODEL", "glm-4.7-flash")
        self.base_system_prompt, self.bio = load_info(user_openid)
        self.system_prompt = self.base_system_prompt

        self.dialog_history, self.summary = load_history(user_openid)

        self.is_compressing = False
        self.compress_callback = None

        # å·¥å…·è°ƒç”¨ç›¸å…³
        self.tools = []
        self._tool_functions = {}
        self.enable_depth_thinking = "disabled" # ç¦ç”¨æ·±åº¦æ€è€ƒ
        self.show_thinking_content = os.getenv("ZHIPU_SHOW_THINKING_CONTENT", "true").lower() == "true"
        self.default_max_tokens = int(os.getenv("QQ_BOT_MAX_TOKENS", "1024"))
        self.default_temperature = float(os.getenv("ZHIPU_DEFAULT_TEMPERATURE", "0.7"))
        self.default_top_p = float(os.getenv("ZHIPU_DEFAULT_TOP_P", "0.7"))
        # åŠ è½½å·¥å…·
        self.load_tools_from_file("message_push/QQ/bot_tool.json")

    async def send_message(self, content: str, msg_type: str = "c2c", max_retries: int = 3) -> dict:
        if self.msg_api:
            try:
                messages = [content]
                if r"||" in content:
                    temp_messages = []
                    for msg in messages:
                        temp_messages.extend(msg.split(r"||"))
                    messages = temp_messages
                if "\n\n" in content:
                    temp_messages = []
                    for msg in messages:
                        temp_messages.extend(msg.split("\n\n"))
                    messages = temp_messages
                messages = [msg.strip() for msg in messages if msg.strip()]

                for i, msg in enumerate(messages):
                    if msg.strip():
                        await self.msg_api.post_c2c_message(
                            openid=self.user_openid,
                            msg_type=0,
                            content=msg.strip(),
                            msg_seq=i + 1
                        )
                        if i < len(messages) - 1:
                            await asyncio.sleep(0.5)
                return {"success": True}
            except Exception as e:
                return {"success": False, "error": str(e)}
        else:
            from message_push.QQ.qq_bot_push import send_notification_with_health_check
            return await send_notification_with_health_check(self.user_openid, content, msg_type)

    def register_tool_function(self, name, func):
        """æ³¨å†Œå·¥å…·å‡½æ•°"""
        self._tool_functions[name] = func

    def register_tool_functions_from_module(self, module):
        """ä»æ¨¡å—æ³¨å†Œæ‰€æœ‰å‡½æ•°"""
        for name, obj in inspect.getmembers(module, inspect.isfunction):
            self._tool_functions[name] = obj

    def load_tools_from_file(self, file_path):
        """ä»JSONæ–‡ä»¶åŠ è½½å·¥å…·é…ç½®"""
        if not os.path.exists(file_path):
            return
        with open(file_path, "r", encoding="utf-8") as f:
            new_tools = json.load(f)
            if isinstance(new_tools, list):
                self.tools.extend(new_tools)
            else:
                self.tools.append(new_tools)

    async def _execute_func(self, func, args):
        """æ‰§è¡Œå·¥å…·å‡½æ•°"""
        if isinstance(args, str):
            args = json.loads(args)
        def truncate_value(v):
            if isinstance(v, list):
                v_str = str(v)
                return v_str[:15] + 'Â·Â·Â·' if len(v_str) > 15 else v
            elif isinstance(v, str):
                return v[:15] + 'Â·Â·Â·' if len(v) > 15 else v
            else:
                v_str = str(v)
                return v_str[:15] + 'Â·Â·Â·' if len(v_str) > 15 else v
        args_short = {k: truncate_value(v) for k, v in args.items()}
        print(f"\n\nğŸ”§ æ‰§è¡Œå·¥å…·å‡½æ•°: {func.__name__} å‚æ•°: {args_short}")
        
        # å‘é€QQé€šçŸ¥
        await self.send_message(f"ğŸ”§ï¼š{func.__name__}")
        
        return func(**args)

    def _serialize_result(self, result):
        """åºåˆ—åŒ–å‡½æ•°è¿”å›ç»“æœ"""
        if result is None:
            return None
        if hasattr(result, '__class__') and result.__class__.__name__ == 'DataFrame':
            return self._serialize_result(result.to_dict('records'))
        if hasattr(result, '__class__') and result.__class__.__name__ == 'Series':
            return self._serialize_result(result.to_dict())
        if hasattr(result, '__class__') and 'ndarray' in result.__class__.__name__:
            return self._serialize_result(result.tolist())
        if hasattr(result, '__class__') and result.__class__.__name__ == 'Timestamp':
            return str(result)
        if isinstance(result, (str, int, float, bool)):
            return result
        if isinstance(result, list):
            return [self._serialize_result(item) for item in result]
        if isinstance(result, dict):
            return {k: self._serialize_result(v) for k, v in result.items()}
        return str(result)

    def _process_stream_response(self, response):
        """å¤„ç†æµå¼å“åº”"""
        reasoning_content = ""
        content = ""
        final_tool_calls = {}
        reasoning_started = False
        content_started = False
        tool_call_started = False

        for chunk in response:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta

            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                if not reasoning_started and delta.reasoning_content.strip():
                    print("\nğŸ§  æ€è€ƒè¿‡ç¨‹ï¼š")
                    reasoning_started = True
                reasoning_content += delta.reasoning_content
                print(delta.reasoning_content, end="", flush=True)

            if hasattr(delta, 'content') and delta.content:
                if not content_started and delta.content.strip():
                    print("\n\nğŸ’¬ å›ç­”å†…å®¹ï¼š")
                    content_started = True
                content += delta.content
                print(delta.content, end="", flush=True)

            if hasattr(delta, 'tool_calls') and delta.tool_calls:
                if not tool_call_started:
                    tool_call_started = True
                for tool_call in delta.tool_calls:
                    index = tool_call.index
                    if index not in final_tool_calls:
                        final_tool_calls[index] = {
                            'id': tool_call.id,
                            'type': tool_call.type,
                            'function': {
                                'name': tool_call.function.name,
                                'arguments': tool_call.function.arguments
                            }
                        }
                    else:
                        final_tool_calls[index]['function']['arguments'] += tool_call.function.arguments

        return reasoning_content, content, final_tool_calls

    def _build_context(self) -> list:
        """æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆsystem + å¯¹è¯ï¼‰"""
        context = []
        system = f""""
        {self.system_prompt}
        ## ç”¨æˆ·ä¿¡æ¯ï¼š
        id:{self.user_openid}
        ### memory:\n{self.bio}
        """
        context.append({"role": "system", "content": system})
        context.extend(self.dialog_history)
        return context

    def set_compress_callback(self, callback):
        """è®¾ç½®å‹ç¼©å®Œæˆå›è°ƒ"""
        self.compress_callback = callback

    async def check_and_compress(self) -> bool:
        """æ£€æŸ¥å¹¶æ‰§è¡Œå‹ç¼©"""
        context = self._build_context()
        tokens = calculate_context_tokens(context)
        if tokens < COMPRESS_THRESHOLD:
            return False

        self.is_compressing = True
        try:
            from message_push.QQ.ai_chat_compress import compress_context
            summary = compress_context(context)
            self.summary = summary
            
            summary_message = {"role": "assistant", "content": f"ã€å†å²å¯¹è¯æ‘˜è¦ã€‘\n{summary}"}
            save_history(self.user_openid, [summary_message])
            self.dialog_history = [summary_message]
            
            self.is_compressing = False
            if self.compress_callback:
                await self.compress_callback("âœ… ä¸Šä¸‹æ–‡å‹ç¼©å®Œæˆï¼å·²ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­å¯¹è¯äº†~")
            return True
        except Exception as e:
            self.is_compressing = False
            if self.compress_callback:
                await self.compress_callback(f"âš ï¸ ä¸Šä¸‹æ–‡å‹ç¼©å¤±è´¥: {e}")
            return False

    async def chat(self, message: str, cancel_event: asyncio.Event = None) -> dict:
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
        ä»…ä½¿ç”¨æ–‡æœ¬æ¨¡å‹ QQCHAT_TEXT_MODEL (glm-4.7-flash)
        å¿½ç•¥æ‰€æœ‰å›¾ç‰‡å†…å®¹
        æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå•è½®å¯¹è¯ï¼Œæ— whileå¾ªç¯ï¼‰
        
        ä½¿ç”¨APIå¯†é’¥ç®¡ç†å™¨å¤„ç†å¹¶å‘è¯·æ±‚
        
        Args:
            cancel_event: ç”¨äºå–æ¶ˆè¯·æ±‚çš„äº‹ä»¶
        
        Returns:
            dict: {"text": å›å¤æ–‡æœ¬}
        """
        if self.is_compressing:
            return {"text": "â³ æ­£åœ¨å‹ç¼©å†å²å¯¹è¯ï¼Œè¯·ç¨ç­‰..."}

        if cancel_event is None:
            cancel_event = asyncio.Event()

        model = self.text_model

        compress_result = await self.check_and_compress()
        if compress_result:
            return {"text": "ğŸ”„ æ£€æµ‹åˆ°å¯¹è¯å†å²è¾ƒé•¿ï¼Œæ­£åœ¨è‡ªåŠ¨å‹ç¼©ä¸Šä¸‹æ–‡..."}

        user_message = {"role": "user", "content": message}
        self.dialog_history.append(user_message)

        context = self._build_context()

        current_task = asyncio.current_task()

        try:
            if cancel_event.is_set():
                return {"text": "", "cancelled": True}

            # æ„å»ºAPIå‚æ•°
            api_params = {
                "model": model,
                "messages": context,
                "max_tokens": self.default_max_tokens,
                "temperature": self.default_temperature,
                "top_p": self.default_top_p,
                "stream": True,
                "thinking": {"type": self.enable_depth_thinking}
            }
            
            if self.tools:
                api_params["tools"] = self.tools
                api_params["tool_choice"] = "auto"

            # ä½¿ç”¨æ”¯æŒå¯†é’¥è½®æ¢çš„APIè°ƒç”¨
            response = await _call_zhipu_api_with_rotation(model, context, api_params)
            
            reasoning_content, content, final_tool_calls = self._process_stream_response(response)

            if reasoning_content:
                self.dialog_history.append({
                    "role": "assistant",
                    "content": reasoning_content
                })

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶ç»§ç»­
            if final_tool_calls:
                for index, tool_call in final_tool_calls.items():
                    function_name = tool_call['function']['name']
                    function_args = tool_call['function']['arguments']

                    func_ref = self._tool_functions.get(function_name)
                    if not func_ref:
                        func_ref = globals().get(function_name)

                    if func_ref:
                        result = await self._execute_func(func_ref, function_args)
                        serialized_result = self._serialize_result(result)
                        self.dialog_history.append({
                            "role": "tool",
                            "content": json.dumps(serialized_result, ensure_ascii=False),
                            "tool_call_id": tool_call['id']
                        })
                    else:
                        self.dialog_history.append({
                            "role": "tool",
                            "content": json.dumps({"error": f"Function {function_name} not found"}, ensure_ascii=False),
                            "tool_call_id": tool_call['id']
                        })
                        print(f"\n\nâš ï¸å‡½æ•° {function_name} æœªæ‰¾åˆ°")

                # æ‰§è¡Œå·¥å…·åï¼Œå†æ¬¡è°ƒç”¨APIè·å–æœ€ç»ˆå›å¤
                context = self._build_context()
                api_params["messages"] = context
                response = await _call_zhipu_api_with_rotation(model, context, api_params)
                reasoning_content, content, final_tool_calls = self._process_stream_response(response)

                if reasoning_content:
                    self.dialog_history.append({
                        "role": "assistant",
                        "content": reasoning_content
                    })

            if content:
                reply = content.strip() if content else ""
                reply = re.sub(r'</?\w+>', '', reply)
                self.dialog_history.append({"role": "assistant", "content": reply})
                save_history(self.user_openid, self.dialog_history)
                return {"text": reply}
            else:
                # æ²¡æœ‰å›å¤ï¼Œå†æ¬¡è¯·æ±‚
                context = self._build_context()
                api_params["messages"] = context
                response = await _call_zhipu_api_with_rotation(model, context, api_params)
                reasoning_content, content, final_tool_calls = self._process_stream_response(response)

                if reasoning_content:
                    self.dialog_history.append({
                        "role": "assistant",
                        "content": reasoning_content
                    })

                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶ç»§ç»­
                if final_tool_calls:
                    for index, tool_call in final_tool_calls.items():
                        function_name = tool_call['function']['name']
                        function_args = tool_call['function']['arguments']

                        func_ref = self._tool_functions.get(function_name)
                        if not func_ref:
                            func_ref = globals().get(function_name)

                        if func_ref:
                            result = await self._execute_func(func_ref, function_args)
                            serialized_result = self._serialize_result(result)
                            self.dialog_history.append({
                                "role": "tool",
                                "content": json.dumps(serialized_result, ensure_ascii=False),
                                "tool_call_id": tool_call['id']
                            })
                        else:
                            self.dialog_history.append({
                                "role": "tool",
                                "content": json.dumps({"error": f"Function {function_name} not found"}, ensure_ascii=False),
                                "tool_call_id": tool_call['id']
                            })
                            print(f"\n\nâš ï¸å‡½æ•° {function_name} æœªæ‰¾åˆ°")

                    # æ‰§è¡Œå·¥å…·åï¼Œå†æ¬¡è°ƒç”¨APIè·å–æœ€ç»ˆå›å¤
                    context = self._build_context()
                    api_params["messages"] = context
                    response = await _call_zhipu_api_with_rotation(model, context, api_params)
                    reasoning_content, content, final_tool_calls = self._process_stream_response(response)

                    if reasoning_content:
                        self.dialog_history.append({
                            "role": "assistant",
                            "content": reasoning_content
                        })

                if content:
                    reply = content.strip() if content else ""
                    reply = re.sub(r'</?\w+>', '', reply)
                    self.dialog_history.append({"role": "assistant", "content": reply})
                    save_history(self.user_openid, self.dialog_history)
                    return {"text": reply}
                else:
                    return {"text": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å›å¤ã€‚"}

        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "1302" in error_str or "1305" in error_str or "è¯·æ±‚è¿‡å¤š" in error_str or "é€Ÿç‡é™åˆ¶" in error_str:
                return {"text": "â³ APIè¯·æ±‚ç¹å¿™ï¼Œè¯·ç¨åå†è¯•~"}
            raise


_sessions = {}
_pending_messages = {}
_pending_lock = asyncio.Lock()


async def queue_user_message(user_openid: str, message: str) -> None:
    """å°†ç”¨æˆ·æ¶ˆæ¯åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—"""
    async with _pending_lock:
        if user_openid not in _pending_messages:
            _pending_messages[user_openid] = []
        _pending_messages[user_openid].append({
            "message": message
        })


async def get_pending_messages(user_openid: str) -> list:
    """è·å–å¹¶æ¸…ç©ºç”¨æˆ·å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        messages = _pending_messages.get(user_openid, []).copy()
        _pending_messages[user_openid] = []
        return messages


async def clear_pending_messages(user_openid: str) -> None:
    """æ¸…ç©ºç”¨æˆ·å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        _pending_messages[user_openid] = []


async def has_pending_messages(user_openid: str) -> bool:
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        return len(_pending_messages.get(user_openid, [])) > 0


async def chat_with_user(user_openid: str, message: str, compress_callback=None,
                         cancel_event: asyncio.Event = None, msg_api=None) -> dict:
    """
    ä¸æŒ‡å®šç”¨æˆ·å¯¹è¯
    
    å¦‚æœç”¨æˆ·æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ï¼Œæ–°æ¶ˆæ¯ä¼šè¢«åŠ å…¥é˜Ÿåˆ—ç­‰å¾…å¤„ç†
    
    Args:
        user_openid: ç”¨æˆ·openid
        message: ç”¨æˆ·æ¶ˆæ¯
        compress_callback: å‹ç¼©å›è°ƒå‡½æ•°
        cancel_event: å–æ¶ˆäº‹ä»¶
        msg_api: æ¶ˆæ¯APIå¯¹è±¡
    
    Returns:
        dict: {"text": å›å¤æ–‡æœ¬}
    """
    if user_openid not in _sessions:
        _sessions[user_openid] = ChatAI(user_openid, msg_api)

    if compress_callback:
        _sessions[user_openid].set_compress_callback(compress_callback)

    if cancel_event is None:
        cancel_event = asyncio.Event()

    pending = await get_pending_messages(user_openid)
    if pending:
        message_parts = [message] if message else []
        for p in pending:
            if p["message"]:
                message_parts.append(p["message"])
        combined_message = "\n".join(message_parts)
        await clear_pending_messages(user_openid)
        return await _sessions[user_openid].chat(combined_message, cancel_event)

    return await _sessions[user_openid].chat(message, cancel_event)


def get_api_status():
    """è·å–APIå¯†é’¥çŠ¶æ€ï¼ˆä¾›è°ƒè¯•ï¼‰"""
    return api_key_manager.get_status()
