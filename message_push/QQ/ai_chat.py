"""QQæœºå™¨äººAIå¯¹è¯æ¨¡å— - æ”¯æŒGLM-4.6Vå¤šæ¨¡æ€ã€å¤šAPIå¯†é’¥ç®¡ç†"""

import os
import json
import base64
import re
import asyncio
from dotenv import load_dotenv
from zhipuai import ZhipuAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥APIå¯†é’¥ç®¡ç†å™¨
from message_push.QQ.api_key_manager import (
    api_key_manager, get_wait_time_estimate
)

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HISTORY_DIR = os.path.join(BASE_DIR, "chat_history")
SYSTEM_PROMPT_FILE = os.path.join(BASE_DIR, "system_prompt.txt")

# å‹ç¼©é˜ˆå€¼ï¼ˆtokenæ•°ï¼Œå¤§çº¦ä¼°ç®—ï¼‰
COMPRESS_THRESHOLD = 10000

# ç¡®ä¿å†å²è®°å½•ç›®å½•å­˜åœ¨
os.makedirs(HISTORY_DIR, exist_ok=True)


def load_system_prompt() -> str:
    """ä»æ–‡ä»¶åŠ è½½system prompt"""
    if os.path.exists(SYSTEM_PROMPT_FILE):
        with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
            return f.read().strip()
    return "ä½ æ˜¯QQç¾¤é‡Œçš„èŠå¤©æœºå™¨äººï¼Œæ€§æ ¼æ´»æ³¼å¯çˆ±ï¼Œå–œæ¬¢ç”¨è¡¨æƒ…åŒ…ã€‚è¯·ç”¨ç®€æ´å‹å¥½çš„ä¸­æ–‡å›ç­”ã€‚"


def load_history(user_openid: str) -> tuple[list, str]:
    """
    åŠ è½½ç”¨æˆ·å†å²å¯¹è¯
    
    Returns:
        (å¯¹è¯è®°å½•åˆ—è¡¨, å‹ç¼©æ‘˜è¦)
    """
    history_file = os.path.join(HISTORY_DIR, f"{user_openid}.json")
    if os.path.exists(history_file):
        with open(history_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data.get("history", []), data.get("summary", "")
            return data, ""
    return [], ""


def save_history(user_openid: str, history: list, summary: str = None):
    """
    ä¿å­˜ç”¨æˆ·å†å²å¯¹è¯
    
    Args:
        history: å¯¹è¯è®°å½•åˆ—è¡¨ï¼ˆä¸åŒ…å«systemï¼‰
        summary: å‹ç¼©æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    """
    history_file = os.path.join(HISTORY_DIR, f"{user_openid}.json")
    
    if summary:
        data = {"history": history, "summary": summary}
    else:
        data = history
    
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def estimate_tokens(text: str) -> int:
    """ä¼°ç®—tokenæ•°"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_words = len(re.findall(r'[a-zA-Z]+', text))
    others = len(text) - chinese_chars - sum(len(w) for w in re.findall(r'[a-zA-Z]+', text))
    return int(chinese_chars * 1.5 + english_words * 1.2 + others * 0.5)


def calculate_context_tokens(context: list) -> int:
    """è®¡ç®—ä¸Šä¸‹æ–‡çš„tokenæ•°"""
    total = 0
    for msg in context:
        content = msg.get("content", "")
        if isinstance(content, str):
            total += estimate_tokens(content)
        elif isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total += estimate_tokens(item.get("text", ""))
                else:
                    total += 1000
    return total


async def _call_zhipu_api(client: ZhipuAI, model: str, messages: list) -> any:
    """
    è°ƒç”¨æ™ºè°±APIï¼ˆå¼‚æ­¥åŒ…è£…ï¼‰
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        None,
        lambda: client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=2048,
            temperature=0.7,
            top_p=0.7,
            stream=False,
            thinking={"type": "disabled"}
        )
    )


class ChatAI:
    """å¤šæ¨¡æ€AIå¯¹è¯ç±» - æ”¯æŒGLM-4.6Vã€å¤šAPIå¯†é’¥ç®¡ç†"""

    def __init__(self, user_openid: str):
        self.user_openid = user_openid
        self.text_model = os.getenv("QQCHAT_TEXT_MODEL", "glm-4.7-flash")
        self.base_system_prompt = load_system_prompt()
        self.system_prompt = self.base_system_prompt

        self.dialog_history, self.summary = load_history(user_openid)

        self.is_compressing = False
        self.compress_callback = None

    def _build_context(self) -> list:
        """æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆsystem + å¯¹è¯ï¼‰"""
        context = []
        
        context.append({"role": "system", "content": self.system_prompt})
        context.extend(self.dialog_history)
        return context

    def set_compress_callback(self, callback):
        """è®¾ç½®å‹ç¼©å®Œæˆå›è°ƒ"""
        self.compress_callback = callback

    async def check_and_compress(self) -> bool:
        """æ£€æŸ¥å¹¶æ‰§è¡Œå‹ç¼©"""
        context = self._build_context()
        tokens = calculate_context_tokens(context)
        if tokens < COMPRESS_THRESHOLD:
            return False

        self.is_compressing = True
        try:
            from message_push.QQ.ai_chat_compress import compress_context
            summary = compress_context(context)
            self.summary = summary
            
            summary_message = {"role": "assistant", "content": f"ã€å†å²å¯¹è¯æ‘˜è¦ã€‘\n{summary}"}
            save_history(self.user_openid, [summary_message])
            self.dialog_history = [summary_message]
            
            self.is_compressing = False
            if self.compress_callback:
                await self.compress_callback("âœ… ä¸Šä¸‹æ–‡å‹ç¼©å®Œæˆï¼å·²ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­å¯¹è¯äº†~")
            return True
        except Exception as e:
            self.is_compressing = False
            if self.compress_callback:
                await self.compress_callback(f"âš ï¸ ä¸Šä¸‹æ–‡å‹ç¼©å¤±è´¥: {e}")
            return False

    async def chat(self, message: str, cancel_event: asyncio.Event = None) -> dict:
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
        ä»…ä½¿ç”¨æ–‡æœ¬æ¨¡å‹ QQCHAT_TEXT_MODEL (glm-4.7-flash)
        å¿½ç•¥æ‰€æœ‰å›¾ç‰‡å†…å®¹
        
        ä½¿ç”¨APIå¯†é’¥ç®¡ç†å™¨å¤„ç†å¹¶å‘è¯·æ±‚
        
        Args:
            cancel_event: ç”¨äºå–æ¶ˆè¯·æ±‚çš„äº‹ä»¶
        
        Returns:
            dict: {"text": å›å¤æ–‡æœ¬}
        """
        if self.is_compressing:
            return {"text": "â³ æ­£åœ¨å‹ç¼©å†å²å¯¹è¯ï¼Œè¯·ç¨ç­‰..."}

        if cancel_event is None:
            cancel_event = asyncio.Event()

        model = self.text_model

        compress_result = await self.check_and_compress()
        if compress_result:
            return {"text": "ğŸ”„ æ£€æµ‹åˆ°å¯¹è¯å†å²è¾ƒé•¿ï¼Œæ­£åœ¨è‡ªåŠ¨å‹ç¼©ä¸Šä¸‹æ–‡..."}

        user_message = {"role": "user", "content": message}

        self.dialog_history.append(user_message)

        context = self._build_context()

        status = api_key_manager.get_status()
        if status["is_full"] and status["queue_size"] == 0:
            pass

        key_info = await api_key_manager.get_api_key()

        max_retries = 3
        retry_delay = 4
        response = None
        last_error = None
        current_task = asyncio.current_task()

        if current_task:
            api_key_manager.register_user_request(self.user_openid, current_task, key_info.name)

        try:
            for attempt in range(max_retries):
                try:
                    if cancel_event.is_set():
                        api_key_manager.unregister_user_request(self.user_openid)
                        return {"text": "", "cancelled": True}

                    client = ZhipuAI(api_key=key_info.key)
                    response = await _call_zhipu_api(client, model, context)
                    api_key_manager.mark_success(key_info)
                    break
                except asyncio.CancelledError:
                    api_key_manager.unregister_user_request(self.user_openid)
                    raise
                except Exception as e:
                    last_error = str(e)
                    api_key_manager.mark_error(key_info, last_error)
                    if "429" in last_error or "1305" in last_error or "è¯·æ±‚è¿‡å¤š" in last_error:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay * (attempt + 1))
                            continue
                    raise
        finally:
            api_key_manager.unregister_user_request(self.user_openid)
            await api_key_manager.release_api_key(key_info)
        
        if response is None:
            return {"text": "â³ APIè¯·æ±‚ç¹å¿™ï¼Œè¯·ç¨åå†è¯•~"}

        reply = response.choices[0].message.content
        reply = reply.strip() if reply else ""
        
        reply = re.sub(r'</?\w+>', '', reply)

        self.dialog_history.append({"role": "assistant", "content": reply})

        save_history(self.user_openid, self.dialog_history)

        return {"text": reply}


_sessions = {}
_pending_messages = {}
_pending_lock = asyncio.Lock()


async def queue_user_message(user_openid: str, message: str) -> None:
    """å°†ç”¨æˆ·æ¶ˆæ¯åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—"""
    async with _pending_lock:
        if user_openid not in _pending_messages:
            _pending_messages[user_openid] = []
        _pending_messages[user_openid].append({
            "message": message
        })


async def get_pending_messages(user_openid: str) -> list:
    """è·å–å¹¶æ¸…ç©ºç”¨æˆ·å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        messages = _pending_messages.get(user_openid, []).copy()
        _pending_messages[user_openid] = []
        return messages


async def clear_pending_messages(user_openid: str) -> None:
    """æ¸…ç©ºç”¨æˆ·å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        _pending_messages[user_openid] = []


async def has_pending_messages(user_openid: str) -> bool:
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰å¾…å¤„ç†æ¶ˆæ¯"""
    async with _pending_lock:
        return len(_pending_messages.get(user_openid, [])) > 0


async def chat_with_user(user_openid: str, message: str, compress_callback=None,
                         cancel_event: asyncio.Event = None) -> dict:
    """
    ä¸æŒ‡å®šç”¨æˆ·å¯¹è¯
    
    å¦‚æœç”¨æˆ·æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ï¼Œæ–°æ¶ˆæ¯ä¼šè¢«åŠ å…¥é˜Ÿåˆ—ç­‰å¾…å¤„ç†
    
    Returns:
        dict: {"text": å›å¤æ–‡æœ¬}
    """
    if user_openid not in _sessions:
        _sessions[user_openid] = ChatAI(user_openid)

    if compress_callback:
        _sessions[user_openid].set_compress_callback(compress_callback)

    if cancel_event is None:
        cancel_event = asyncio.Event()

    pending = await get_pending_messages(user_openid)
    if pending:
        message_parts = [message] if message else []
        for p in pending:
            if p["message"]:
                message_parts.append(p["message"])
        combined_message = "\n".join(message_parts)
        await clear_pending_messages(user_openid)
        return await _sessions[user_openid].chat(combined_message, cancel_event)

    return await _sessions[user_openid].chat(message, cancel_event)


def get_api_status():
    """è·å–APIå¯†é’¥çŠ¶æ€ï¼ˆä¾›è°ƒè¯•ï¼‰"""
    return api_key_manager.get_status()
